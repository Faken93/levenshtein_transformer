{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pytorch_transformers.tokenization_distilbert import DistilBertTokenizer\n",
    "from pytorch_transformers.modeling_distilbert import DistilBertModel\n",
    "import torch\n",
    "import torch.nn\n",
    "from torch import optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "special_tokens_dict = {'additional_special_tokens': ['<PLH>']}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "encoder = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "noop = \"N\"\n",
    "sub = \"S\"\n",
    "insert = \"I\"\n",
    "delete = \"D\"\n",
    "\n",
    "def ld(s1, s2, subcost=1, delcost=1, inscost=1):\n",
    "    operations = [[\"\" for j in range(len(s2) + 1)] for i in range(len(s1) + 1)]\n",
    "    \n",
    "    matrix = np.zeros((len(s1)+1, len(s2)+1))\n",
    "    \n",
    "    for j in range(len(s2) + 1):\n",
    "        matrix[0,j] = j\n",
    "        operations[0][j] = insert\n",
    "        for i in range(len(s1) + 1):\n",
    "            matrix[i,0] = i\n",
    "            operations[i][0] = insert\n",
    "            if i > 0 and j > 0:\n",
    "                subCost = matrix[i-1, j-1] if s1[i-1] == s2[j-1] else matrix[i-1, j-1] + subcost\n",
    "                insertCost = matrix[i, j-1] + inscost\n",
    "                deleteCost = matrix[i-1, j] + delcost\n",
    "                minCost = min(subCost, insertCost, deleteCost)\n",
    "                matrix[i,j] = minCost\n",
    "                if minCost == 0:\n",
    "                    operations[i][j] = noop\n",
    "                elif minCost == deleteCost:\n",
    "                    operations[i][j] = delete\n",
    "                elif minCost == insertCost:\n",
    "                    operations[i][j] = insert\n",
    "                elif minCost == subCost:\n",
    "                    operations[i][j] = sub\n",
    "    i = len(s1)\n",
    "    j = len(s2)\n",
    "    history = []\n",
    "    while j > 0 or i > 0:        \n",
    "        if delcost != np.inf:\n",
    "            if j == 0:\n",
    "                history.append(delete)\n",
    "                i -= 1\n",
    "                continue\n",
    "            if matrix[i-1][j-1] < matrix[i-1,j]:\n",
    "                history.append(noop)\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "            else:\n",
    "                history.append(delete)\n",
    "                i -= 1\n",
    "        elif inscost != np.inf:\n",
    "            if j == 0:\n",
    "                history.append(noop)\n",
    "                i -= 1\n",
    "                continue\n",
    "            if matrix[i-1][j-1] < matrix[i,j-1]:\n",
    "                history.append(noop)\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "            else:\n",
    "                history.append((insert,s2[j-1]))\n",
    "                #history.append(insert)\n",
    "                j -= 1\n",
    "    history.reverse()\n",
    "    return matrix, matrix[len(s1),len(s2)], history\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        with open(path, \"r\") as infile:\n",
    "            self.data = json.load(infile)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[\"Data\"])\n",
    "\n",
    "    def sample(self):\n",
    "        return self.data[\"Data\"][random.randint(0, len(self.data[\"Data\"]))][\"Question\"]\n",
    "    \n",
    "from torch import nn\n",
    "\n",
    "PLH = \"<PLH>\"\n",
    "TS = \"<s>\"\n",
    "TE = \"</s>\"\n",
    "\n",
    "class PlaceholderClassifier(nn.Module):\n",
    "    def __init__(self, hsz, max_placeholders=10):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(\n",
    "            hsz, max_placeholders,\n",
    "        )\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        return self.activation(self.dense(input))\n",
    "    \n",
    "class TokenClassifier(nn.Module):    \n",
    "    def __init__(self, hsz, vsz, max_seq_len):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dense = nn.Linear(\n",
    "            hsz, vsz\n",
    "        )\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, input, hidden=None):\n",
    "        return self.activation(self.dense(input))\n",
    "        \n",
    "class DeletionClassifier(nn.Module):    \n",
    "    def __init__(self, hsz):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(\n",
    "            hsz, 2,\n",
    "        )\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, input, hidden=None):\n",
    "        return self.activation(self.dense(input))\n",
    "dataset = QADataset(\"/virtualmachines/data/trivia_qa/qa/wikipedia-train.json\")\n",
    "dataset.sample()\n",
    "plh = tokenizer.encode(\"<PLH>\")[0]\n",
    "plh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[7592, 1010, 2171], [2054, 2003, 2171, 1029]],\n",
       " [[7592, 1010, 30522, 2171, 30522], [2054, 2003, 30522, 2171, 1029]],\n",
       " [[0, 0, 1, 0, 1], [0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def deleted_indices_to_placeholders(deleted):\n",
    "    i = 0\n",
    "    placeholders = []\n",
    "    num_deleted = 0\n",
    "    while True:\n",
    "        if i == len(deleted):\n",
    "            if num_deleted > 0:\n",
    "                placeholders.append(num_deleted)\n",
    "            break\n",
    "        if deleted[i] == 1:\n",
    "            num_deleted += 1\n",
    "        else:\n",
    "            if num_deleted > 0:\n",
    "                placeholders.append(num_deleted)\n",
    "            placeholders.append(0)\n",
    "            num_deleted = 0\n",
    "        i += 1\n",
    "    while len(placeholders) < len(deleted):\n",
    "        placeholders.append(0)\n",
    "    return torch.unsqueeze(torch.LongTensor(placeholders), 0)\n",
    "\n",
    "def apply_deletions(y, deleted, pad=False):\n",
    "#    if pad:\n",
    "#        post_deletion = torch.zeros(y.size(), dtype=torch.int64)\n",
    "#    else:\n",
    "#        post_deletion = torch.zeros(y.size(0), y.size(1) - len(deleted), dtype=torch.int64)\n",
    "#    post_deletion_with_placeholders = torch.zeros(y.size())\n",
    "    #print(\"applying deletions\")\n",
    "    batch_post_deletion = []\n",
    "    batch_post_deletion_with_placeholders = []\n",
    "    for batch_index in range(len(y)):\n",
    "    #    print(y[batch_index])\n",
    "        j = 0\n",
    "        post_deletion = []\n",
    "        post_deletion_with_placeholders = []\n",
    "        for i in range(len(y[batch_index])):\n",
    "            if deleted[batch_index][i] == 0:\n",
    "                post_deletion.append(y[batch_index][i])\n",
    "                post_deletion_with_placeholders.append(y[batch_index][i])\n",
    "                j += 1\n",
    "            else:\n",
    "                post_deletion_with_placeholders.append(plh)\n",
    "        batch_post_deletion.append(post_deletion)\n",
    "        batch_post_deletion_with_placeholders.append(post_deletion_with_placeholders)\n",
    "    return batch_post_deletion, batch_post_deletion_with_placeholders\n",
    "\n",
    "def delete_random(y, p=0.25, pad=True):\n",
    "    \"\"\"Deletes token(s) randomly from the passed (tokenized) string with probability p\n",
    "    Accepts:\n",
    "    - a list of token sequences bsz * pad_length\n",
    "    Returns tensors of:\n",
    "    - the token sequence post-deletion\n",
    "    - the token sequence post-deletion, with PLH inserted at each deleted position\n",
    "    - the number of placeholders inserted at each post-deletion index\n",
    "    \"\"\"\n",
    "    batch_deletions = []\n",
    "    for i in range(len(y)):\n",
    "        deletions = []\n",
    "        for i in range(len(y[i])):\n",
    "            if random.random() < p:\n",
    "                deletions.append(1)\n",
    "            else:\n",
    "                deletions.append(0)\n",
    "        batch_deletions.append(deletions)\n",
    "    post_deletion, post_deletion_with_placeholders = apply_deletions(y, batch_deletions)\n",
    "    #placeholders = deleted_indices_to_placeholders(deleted)\n",
    "    #assert placeholders.size() == post_deletion.size()\n",
    "    return post_deletion, post_deletion_with_placeholders, batch_deletions#,placeholders\n",
    "\n",
    "delete_random([tokenizer.encode(\"Hello, my name is \"), tokenizer.encode(\"What is your name?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0, 0, 0, 0, 5], [0, 0, 0, 0, 5]],\n",
       " [[1010, 2054, 2003, 2115, 2171], [3899, 22953, 2017, 2360, 2085]],\n",
       " tensor([[ 2026,  2171,  2003,  4172,  1010,  2054,  2003,  2115,  2171],\n",
       "         [ 2054,  1005,  1055,  1996,  3899, 22953,  2017,  2360,  2085]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def delete_minimal(y, y_ground, pad=True):\n",
    "    \"\"\"Apply the sequence of deletions from y that give the smallest possible Levenshtein distance from y_ground\n",
    "    Returns tensors of:\n",
    "    - the token sequence post-deletion\n",
    "    - the token sequence post-deletion, with PLH inserted at each deleted position\n",
    "    - the number of placeholders inserted at each post-deletion index\n",
    "    \"\"\"\n",
    "    batched = torch.zeros(len(y),)\n",
    "    \n",
    "    # if y_ground is longer than y, there is no sequence of deletes with a shorter distance\n",
    "    if len(y_ground) > len(y): \n",
    "        return y\n",
    "    \n",
    "    # calculate LD directly against tokens\n",
    "    deleted = []\n",
    "    matrix, dist, edits = ld(y.numpy(),y_ground.numpy(), subcost=np.inf, inscost=np.inf)\n",
    "    num_deleted = 0\n",
    "    for i in range(len(edits)):\n",
    "        if edits[i] == \"D\":\n",
    "            deleted.append(1)\n",
    "            num_deleted += 1\n",
    "        else:\n",
    "            deleted.append(0)\n",
    "    placeholders = deleted_indices_to_placeholders(deleted)\n",
    "    post_deletion, post_deletion_with_placeholders = deleted_boolean_to_tensors(y, deleted, num_deleted)    \n",
    "    return post_deletion, post_deletion_with_placeholders, torch.LongTensor([deleted])#,placeholders\n",
    "\n",
    "def pad(items, pad_len=0, pad_token=0):\n",
    "    if pad_len == 0:\n",
    "        pad_len = max([len(i) for i in items])\n",
    "    for i in items:\n",
    "        while len(i) < pad_len:\n",
    "            i.append(pad_token)\n",
    "    return items\n",
    "    \n",
    "def insert_minimal(y, y_ground):\n",
    "    \"\"\"Apply the sequence of insertions to y resulting in the smallest possible Levenshtein distance from y_ground\n",
    "    Accepts tensor of:\n",
    "    - bsz * max_seq_len\n",
    "    Returns tensors of:\n",
    "    - size (n+1), where the value at position 0 <= i < n represents the number of PLH tags inserted at that position\n",
    "        - n is the number of tokens in y\n",
    "    - size(k) containing the indices of each inserted token, where k is the total number of tokens added\n",
    "    - size(n+k) containing the tokens of the entire post-insertion sequence\n",
    "    \"\"\"\n",
    "    #if y.size(1) >= y_ground.size(1): # if y is larger than y_ground, no sequence of inserts that will give a smaller LD\n",
    "    #    return torch.LongTensor([[0] * y.size(1)]), torch.LongTensor([]), y\n",
    "    #print(\"inserting minimal\")\n",
    "    #print(y)\n",
    "    batch_placeholders = []\n",
    "    batch_inserted = []\n",
    "    batch_new = []\n",
    "    for batch_idx in range(len(y)):\n",
    "        matrix, dist, edits = ld(y[batch_idx],y_ground[batch_idx], subcost=np.inf, delcost=np.inf)\n",
    "        inserted = 0\n",
    "        y_placeholders = []\n",
    "        y_inserted = []\n",
    "        y_new = []\n",
    "        y_idx = 0\n",
    "        i = 0\n",
    "        while i < len(edits):\n",
    "            if edits[i][0] == \"I\":\n",
    "                accum = 0\n",
    "                while i < len(edits) and edits[i][0] == \"I\":\n",
    "                    y_inserted.append(edits[i][1])\n",
    "                    y_new.append(edits[i][1])\n",
    "                    accum += 1\n",
    "                    i += 1\n",
    "                y_placeholders.append(accum)\n",
    "            else:\n",
    "                y_placeholders.append(0)\n",
    "                y_new.append(y[batch_idx][y_idx])\n",
    "                i += 1\n",
    "                y_idx += 1\n",
    "        batch_placeholders.append(y_placeholders)\n",
    "        batch_inserted.append(y_inserted)\n",
    "        batch_new.append(y_new)\n",
    "    batch_placeholders = pad(batch_placeholders, pad_token=tokenizer.pad_token_id)\n",
    "    #batch_inserted = pad(batch_inserted)\n",
    "    batch_new = pad(batch_new, pad_token=tokenizer.pad_token_id)\n",
    "    #print(batch_placeholders)\n",
    "    #print(batch_new)\n",
    "    return batch_placeholders, batch_inserted, torch.LongTensor(batch_new)\n",
    "\n",
    "y1 = tokenizer.encode(\"My name is Nick, what is your name\")\n",
    "y2 = tokenizer.encode(\"My name is Nick\")\n",
    "y3 = tokenizer.encode(\"What's the dog bro you say now\")\n",
    "y4 = tokenizer.encode(\"What's the\")\n",
    "\n",
    "#delete_minimal(torch.LongTensor([y1]), torch.LongTensor([y2]))\n",
    "insert_minimal([y2,y4], [y1,y3])\n",
    "#y = torch.LongTensor([tokenizer.encode(\"Hi dude My name is Nick what name\")])\n",
    "#y_ground = torch.LongTensor([tokenizer.encode(\"My name is Nick, what is your name\")])\n",
    "\n",
    "#placeholders, inserted, new = insert_minimal(y, y_ground)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "tensor(10.3345, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "tensor(10.3515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "tensor(10.3375, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "tensor(10.3449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "tensor(10.3383, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "tensor(10.3247, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "tensor(10.3060, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-bdd93117dafc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-bdd93117dafc>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class DatasetSampler():\n",
    "    def __init__(self, tokenizer, encoder, alpha=0, beta=0):\n",
    "        self.dataset = QADataset(\"/virtualmachines/data/trivia_qa/qa/wikipedia-train.json\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoder = encoder\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "            \n",
    "    def encode_and_pad(self, string, pad_length):\n",
    "        string = self.tokenizer.encode(string)\n",
    "        while len(string) < pad_length:\n",
    "            string.append(self.tokenizer.pad_token_id)\n",
    "        return torch.LongTensor([string])\n",
    "    \n",
    "    '''\n",
    "        Encode the passed strings and pad to the specified length\n",
    "    '''\n",
    "    def encode_and_pad_batch(self, strings):\n",
    "        encoded = [self.tokenizer.encode(s) for s in strings]\n",
    "        pad_length = max([len(s) for s in encoded])\n",
    "        padded = torch.zeros(len(strings), pad_length, dtype=torch.int64) # because self.tokenizer.pad_token_id == 0\n",
    "        for i in range(len(encoded)):\n",
    "            for j in range(len(encoded[i])):\n",
    "                padded[i,j] = encoded[i][j]\n",
    "        return padded\n",
    "        \n",
    "    def sample(self, bs=10, pad_to=30):\n",
    "        '''Sample an observation and return tensor tuples:\n",
    "        1) (a) the observation perturbed with deletions, for input to the placeholder classifier\n",
    "           (b) the indices of the deleted tokens (i.e. where placeholders should be inserted, for the placeholder classifier loss) \n",
    "        2) (a) 1(a), but with PLH replacing each deleted token. For input to the token insertion classifier \n",
    "           (b) the number of placeholders tokens to insert at each index in 1(a) (for the placeholder classifier loss)\n",
    "        3) (a) the observation perturbed with insertion (tokens), for input to the token deletion classifier \n",
    "           (b) 2(a) (for the token insertion classifier loss)\n",
    "        '''\n",
    "        u = random.random()\n",
    "        v = random.random()\n",
    "        # sample a pair of (untokenized) strings \n",
    "        y_ground = [self.dataset.sample() for i in range(bs)]\n",
    "        y0 = \"\"\n",
    "        \n",
    "        # first, pad/encode y0 and y_ground\n",
    "        y_ground = [self.tokenizer.encode(s) for s in y_ground]\n",
    "        #y_ground = self.encode_and_pad_batch(y_ground)\n",
    "        #y0 = self.encode_and_pad(y0, pad_to)\n",
    "        \n",
    "        y0 = y_ground\n",
    "        #print(\"y0\")\n",
    "        #print(y0)\n",
    "        # randomly choose between LD-minimal deletion and random deletion\n",
    "        # Returns tensors of:\n",
    "        # - the token sequence post-deletion\n",
    "        # - the token sequence post-deletion, with PLH inserted at each deleted position\n",
    "        # - whether a given index was deleted or not\n",
    "        if u >= self.alpha:\n",
    "            y_ins, y_ins_with_placeholders, y_ins_p = delete_random(y0)\n",
    "        else:\n",
    "            y_ins, y_ins_with_placeholders, y_ins_p = delete_minimal(y0, y_ground)\n",
    "        #print(\"y_ins\")\n",
    "        #print(y_ins)\n",
    "        # Returns tensors of:\n",
    "        # - (1, n+1) - the number of PLH tags at each index\n",
    "        # - (1, k) - the inserted tokens\n",
    "        # - (1, n+k) - the entire post-insertion sequence\n",
    "        # where n is the length of the original sequence and k is the number of tokens added \n",
    "        y_placeholders, y_inserted, y_ins_prime = insert_minimal(y_ins, y_ground)\n",
    "\n",
    "        # input to the deletion classifier will be randomly chosen between:\n",
    "        # - the input to the placeholder classifier (i.e. the deleted input)\n",
    "        # - the output from applying the token classifier to y_placeholders\n",
    "        if v >= self.alpha:\n",
    "            y_del = y_ins\n",
    "        else:\n",
    "            enc = self.encoder(y_ins_with_placeholders)[0]\n",
    "            logits = self.t_classifier(enc)\n",
    "            y_del = torch.argmax(logits,dim=2)\n",
    "            y_del = torch.LongTensor(y_del)\n",
    "\n",
    "        # y_ins will be one token shorter than y_placeholders to account for placeholders added to the end of the sequence\n",
    "        #if len(y_ins) != len(y_placeholders):\n",
    "            #y_ins_copy = torch.zeros(y_ins.size(0), y_ins.size(1) + 1, dtype=torch.int64)\n",
    "            #y_ins_copy[:,:y_ins.size(1)] = y_ins\n",
    "            #y_ins = y_ins_copy\n",
    "        #    y_ins.append(0)\n",
    "        y_ins_p = torch.LongTensor(pad(y_ins_p, pad_token=0))\n",
    "        \n",
    "        y_del = torch.LongTensor(pad(y_del, pad_len=y_ins_p.size(1), pad_token=0))\n",
    "        \n",
    "        y_ins = torch.LongTensor(pad(y_ins, pad_token=0))\n",
    "        y_placeholders = torch.LongTensor(pad(y_placeholders, pad_len=y_ins.size(1), pad_token=0))\n",
    "        \n",
    "        y_ins_prime = torch.LongTensor(pad(y_ins_prime, pad_token=0))\n",
    "        \n",
    "        #print(y_ins_prime.size())\n",
    "        #print(y_ins.size())\n",
    "        \n",
    "        return ((self.encoder(y_del)[0], y_ins_p), \n",
    "               (self.encoder(y_ins)[0], y_placeholders), \n",
    "               (self.encoder(y_ins_prime)[0], y_ins))\n",
    "    \n",
    "class Model():\n",
    "    def __init__(self, sampler, vocab_size, hsz=768, lr=0.0001):\n",
    "        super().__init__()\n",
    "        self.sampler = sampler\n",
    "        self.p_classifier = PlaceholderClassifier(hsz)\n",
    "        self.t_classifier = TokenClassifier(hsz, vocab_size, 20)\n",
    "        self.d_classifier = DeletionClassifier(hsz)        \n",
    "        self.alpha = 0.5\n",
    "        self.beta = 0.5\n",
    "        self.p_loss = nn.CrossEntropyLoss()\n",
    "        self.t_loss = nn.CrossEntropyLoss()\n",
    "        self.d_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.optims = {\n",
    "            'p_classifier': optim.SGD(self.p_classifier.parameters(), lr=lr),\n",
    "            't_classifier': optim.SGD(self.t_classifier.parameters(), lr=lr),\n",
    "            'd_classifier': optim.SGD(self.d_classifier.parameters(), lr=lr),\n",
    "        }\n",
    "        \n",
    "        self.step = 0\n",
    "        self.loss = 0\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for optimizer in self.optims.values():\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    def update_params(self):\n",
    "        for optimizer in self.optims.values():\n",
    "            optimizer.step()\n",
    "   \n",
    "    def train_step(self):\n",
    "        loss = 0\n",
    "        self.zero_grad()\n",
    "        self.p_classifier.train()\n",
    "        self.t_classifier.train()\n",
    "        self.d_classifier.train()\n",
    "        \n",
    "        (y_del,y_del_out), (y_ins,y_ins_out), (y_ins_prime, y_ins_prime_out) = self.sampler.sample(bs=10)\n",
    "                \n",
    "        preds_deletes = self.d_classifier(y_del).cuda()\n",
    "        preds_placeholders = self.p_classifier(y_ins).cuda()\n",
    "        #print(y_ins_prime.size())\n",
    "        #print(y_ins_prime)\n",
    "        preds_inserts = self.t_classifier(y_ins_prime).cuda()\n",
    "        #print(preds_inserts)\n",
    "        #print(y_ins_prime_out.size())\n",
    "        \n",
    "        loss = self.d_loss(torch.transpose(preds_deletes, 1, 2), y_del_out.cuda())\n",
    "        loss += self.p_loss(torch.transpose(preds_placeholders, 1, 2), y_ins_out.cuda())\n",
    "        loss += self.t_loss(torch.transpose(preds_inserts,1,2), y_ins_prime_out.cuda())\n",
    "        self.step += 1\n",
    "        self.loss += loss\n",
    "        if self.step % 50 == 0:\n",
    "            print(self.step)\n",
    "            print(self.loss / 50)\n",
    "            self.loss = 0\n",
    "        \n",
    "        loss.backward()\n",
    "        self.update_params()\n",
    "\n",
    "sampler = DatasetSampler(tokenizer, encoder)\n",
    "model = Model(sampler, tokenizer.vocab_size)\n",
    "for i in range(10000):\n",
    "    model.train_step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_random(\"<s>hellodude</s>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
