{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pytorch_transformers.tokenization_distilbert import DistilBertTokenizer\n",
    "from pytorch_transformers.modeling_distilbert import DistilBertModel\n",
    "import torch\n",
    "import torch.nn\n",
    "from torch import optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "encoder = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noop = \"N\"\n",
    "sub = \"S\"\n",
    "insert = \"I\"\n",
    "delete = \"D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ld(s1, s2, subcost=1, delcost=1, inscost=1):\n",
    "    operations = [[\"\" for j in range(len(s2) + 1)] for i in range(len(s1) + 1)]\n",
    "    matrix = np.zeros((len(s1)+1, len(s2)+1))\n",
    "    for j in range(len(s2) + 1):\n",
    "        matrix[0,j] = j\n",
    "        operations[0][j] = insert\n",
    "        for i in range(len(s1) + 1):\n",
    "            matrix[i,0] = i\n",
    "            operations[i][0] = insert\n",
    "            if i > 0 and j > 0:\n",
    "                subCost = matrix[i-1, j-1] if s1[i-1] == s2[j-1] else matrix[i-1, j-1] + subcost\n",
    "                insertCost = matrix[i, j-1] + inscost\n",
    "                deleteCost = matrix[i-1, j] + delcost\n",
    "                #print(\"subCost %d\" % subCost)\n",
    "                minCost = min(subCost, insertCost, deleteCost)\n",
    "                #print(\"minbCost %d\" % minCost)\n",
    "                matrix[i,j] = minCost\n",
    "                if minCost == 0:\n",
    "                    operations[i][j] = noop\n",
    "                elif minCost == deleteCost:\n",
    "                    operations[i][j] = delete\n",
    "                elif minCost == insertCost:\n",
    "                    operations[i][j] = insert\n",
    "                elif minCost == subCost:\n",
    "                    operations[i][j] = sub\n",
    "    i = len(s1)\n",
    "    j = len(s2)\n",
    "    history = []\n",
    "    while j > 0 or i > 0:        \n",
    "        if delcost != np.inf:\n",
    "            if j == 0:\n",
    "                history.append(delete)\n",
    "                i -= 1\n",
    "                continue\n",
    "            if matrix[i-1][j-1] < matrix[i-1,j]:\n",
    "                history.append(noop)\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "            else:\n",
    "                history.append(delete)\n",
    "                i -= 1\n",
    "        elif inscost != np.inf:\n",
    "            if j == 0:\n",
    "                history.append(noop)\n",
    "                i -= 1\n",
    "                continue\n",
    "            if matrix[i-1][j-1] < matrix[i,j-1]:\n",
    "                history.append(noop)\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "            else:\n",
    "                history.append((insert,s2[j-1]))\n",
    "                #history.append(insert)\n",
    "                j -= 1\n",
    "    history.reverse()\n",
    "    return matrix, matrix[len(s1),len(s2)], history\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        with open(path, \"r\") as infile:\n",
    "            self.data = json.load(infile)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[\"Data\"])\n",
    "\n",
    "    def sample(self):\n",
    "        #print(self.data[\"Data\"][random.randint(0, len(self.data[\"Data\"]))])\n",
    "        return self.data[\"Data\"][random.randint(0, len(self.data[\"Data\"]))][\"Question\"]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "PLH = \"<PLH>\"\n",
    "TS = \"<s>\"\n",
    "TE = \"</s>\"\n",
    "\n",
    "class PlaceholderClassifier(nn.Module):\n",
    "    def __init__(self, hsz, max_placeholders=10):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(\n",
    "            hsz, max_placeholders,\n",
    "        )\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        return self.activation(self.dense(input))\n",
    "    \n",
    "class TokenClassifier(nn.Module):    \n",
    "    def __init__(self, hsz, vsz, max_seq_len):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dense = nn.Linear(\n",
    "            hsz\n",
    "        )\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, input, hidden=None):\n",
    "        s = input.sum(dim=1)\n",
    "        return self.activation(self.dense(s))\n",
    "        \n",
    "class DeletionClassifier(nn.Module):    \n",
    "    def __init__(self, hsz):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(\n",
    "            hsz, 2,\n",
    "        )\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, input, hidden=None):\n",
    "        return self.activation(self.dense(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Errol Brown who died last month was best known as a member of which band?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = QADataset(\"/virtualmachines/data/trivia_qa/qa/wikipedia-train.json\")\n",
    "dataset.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'vsz' and 'max_seq_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f2b40590115e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-f2b40590115e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tokenizer, encoder, hsz, alpha, beta, lr)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceholderClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeletionClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'vsz' and 'max_seq_len'"
     ]
    }
   ],
   "source": [
    "class Model():\n",
    "    def __init__(self, tokenizer, encoder, hsz=768, alpha=0, beta=0, lr=0.0001):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoder = encoder\n",
    "        self.p_classifier = PlaceholderClassifier(hsz)\n",
    "        self.t_classifier = TokenClassifier(hsz)\n",
    "        self.d_classifier = DeletionClassifier(hsz)        \n",
    "        self.alpha = 0.5\n",
    "        self.beta = 0.5\n",
    "        self.p_loss = nn.NLLLoss()\n",
    "        self.t_loss = nn.NLLLoss()\n",
    "        self.d_loss = nn.NLLLoss()\n",
    "        \n",
    "        self.optims = {\n",
    "            'p_classifier': optim.SGD(self.p_classifier.parameters(), lr=lr),\n",
    "            't_classifier': optim.SGD(self.t_classifier.parameters(), lr=lr),\n",
    "            'd_classifier': optim.SGD(self.d_classifier.parameters(), lr=lr),\n",
    "        }\n",
    "        \n",
    "        self.dataset = QADataset(\"/virtualmachines/data/trivia_qa/qa/wikipedia-train.json\")\n",
    "\n",
    "        \n",
    "    def delete_random(self, y):\n",
    "        for i in range(3, len(y) - 4): # don't delete <s> or </s>\n",
    "            if random.random() > 0.05:\n",
    "                y[i] = None\n",
    "        return str([yi for yi in y if yi is not None])\n",
    "\n",
    "    def delete_minimal(self, y, y_ground):\n",
    "        if len(y_ground) > len(y): # if y_ground is longer than y, there is no sequence of deletes that will give a smaller LD\n",
    "            return y\n",
    "        y = y[3:len(y) - 4]\n",
    "        y_ground = y_ground[3:len(y_ground) - 4] \n",
    "        matrix, dist, edits = ld(y,y_ground, subcost=np.inf, inscost=np.inf)\n",
    "        deleted = 0\n",
    "        for i in range(len(edits)):\n",
    "            if edits[i] == \"D\":\n",
    "                y = y[:i-deleted] + y[i-deleted+1:]\n",
    "                deleted += 1\n",
    "        return TS + str(y) + TE\n",
    "\n",
    "    def insert_minimal(self, y, y_ground):\n",
    "        if len(y) > len(y_ground): # if y is larger than y_ground, no sequence of inserts that will give a smaller LD\n",
    "            return y\n",
    "        y = y[3:len(y) - 4]\n",
    "        y_ground = y_ground[3:len(y_ground) - 4] \n",
    "        matrix, dist, edits = ld(y,y_ground, subcost=np.inf, delcost=np.inf)\n",
    "        inserted = 0\n",
    "        i = 0\n",
    "        p = y\n",
    "        for edit in edits:\n",
    "            if edit[0] == \"I\":\n",
    "                y = str(y[:i+inserted]) + edit[1] + str(y[i+inserted+1:])\n",
    "                p = str(p[:i+inserted]) + PLH + str(y[i+inserted+1:])\n",
    "                inserted += 1\n",
    "            i += 1\n",
    "        return TS + y + TE, TS + p + TE\n",
    "            \n",
    "\n",
    "    def sample(self, alpha, beta):\n",
    "        u = random.random()\n",
    "        v = random.random()\n",
    "        y_ground = TS + self.dataset.sample() + TE # produce a pair of strings (i.e. untokenized)\n",
    "        y0 = TS + TE\n",
    "        if len(y0) > 2: # \"<s></s>\" is an empty sequence, so no deletion is possible\n",
    "            if u >= alpha:\n",
    "                y_ins = self.delete_random(y0)\n",
    "            else:\n",
    "                y_ins = self.delete_minimal(y0, y_ground)       \n",
    "\n",
    "            y_ins_prime_p,y_ins_prime_t = self.insert_minimal(y_ins, y_ground)\n",
    "            y_ins_prime_p = torch.LongTensor([self.tokenizer.encode(y_ins_prime_p)])\n",
    "            y_ins_prime_t = self.tokenizer.encode(y_ins_prime_t)\n",
    "            y_ins_prime_t = torch.LongTensor([y_ins_prime_t])\n",
    "            \n",
    "            #y_ins_prime_t = self.encoder(y_ins_prime_t)[0]\n",
    "            \n",
    "            y_ins = torch.LongTensor([self.tokenizer.encode(y_ins)])\n",
    "            #y_ins = self.encoder(y_ins)[0]\n",
    "            \n",
    "            if v > alpha:\n",
    "                y_del = y0\n",
    "                y_del = self.tokenizer.encode(y_del)\n",
    "                y_del = torch.LongTensor([y_del])\n",
    "                #y_del = self.encoder(y_del)[0]\n",
    "            else:\n",
    "                logits = self.t_classifier(self.encoder(y_ins_prime_t)[0])\n",
    "                y_del = torch.argmax(logits,dim=2)\n",
    "        \n",
    "        return y_del, y_ins, y_ins_prime_p, y_ins_prime_t\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for optimizer in self.optims.values():\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    def update_params(self):\n",
    "        for optimizer in self.optims.values():\n",
    "            optimizer.step()\n",
    "   \n",
    "    def train_step(self):\n",
    "        y_del, y_ins, y_ins_prime_p, y_ins_prime_t = self.sample(self.alpha, self.beta)\n",
    "        preds_placeholders = self.p_classifier(self.encoder(y_ins)[0])\n",
    "        preds_inserts = self.t_classifier(self.encoder(y_ins_prime_p)[0])\n",
    "        preds_deletes = self.d_classifier(self.encoder(y_del)[0])\n",
    "\n",
    "        loss = 0\n",
    "        self.zero_grad()\n",
    "        self.encoder.train()\n",
    "        \n",
    "        loss = self.d_loss(preds_deletes, y_del)\n",
    "        loss += self.t_loss(preds_inserts, y_ins_prime_t)\n",
    "        loss += self.p_loss(preds_placeholders, y_ins)\n",
    "        loss.backward()\n",
    "        self.update_params()\n",
    "model = Model(tokenizer, encoder)\n",
    "model.train_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'insert_minimal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-8cb05e4f925a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minsert_minimal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<s>fello</s>\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"<s>ellodude</s>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'insert_minimal' is not defined"
     ]
    }
   ],
   "source": [
    "insert_minimal(\"<s>fello</s>\",\"<s>ellodude</s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "delete_minimal() missing 1 required positional argument: 'y_ground'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-7ebc9d21159f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-47-c0e5078b9d3a>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0my_del\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ins_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mpreds_placeholders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_ins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-c0e5078b9d3a>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, alpha, beta)\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0my_ins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0my_ins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_minimal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_ground\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0my_ins_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_minimal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ground\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: delete_minimal() missing 1 required positional argument: 'y_ground'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
