{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pytorch_transformers.tokenization_distilbert import DistilBertTokenizer\n",
    "from pytorch_transformers.modeling_distilbert import DistilBertModel\n",
    "import torch\n",
    "import torch.nn\n",
    "from torch import optim\n",
    "import random\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "from torch import nn\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "special_tokens_dict = {'additional_special_tokens': ['<PLH>']}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "encoder = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "encoder.resize_token_embeddings(len(tokenizer))  \n",
    "\n",
    "noop = \"N\"\n",
    "sub = \"S\"\n",
    "insert = \"I\"\n",
    "delete = \"D\"\n",
    "\n",
    "def ld(s1, s2, subcost=1, delcost=1, inscost=1):\n",
    "    operations = [[\"\" for j in range(len(s2) + 1)] for i in range(len(s1) + 1)]\n",
    "    \n",
    "    matrix = np.zeros((len(s1)+1, len(s2)+1))\n",
    "    \n",
    "    for j in range(len(s2) + 1):\n",
    "        matrix[0,j] = j\n",
    "        operations[0][j] = insert\n",
    "        for i in range(len(s1) + 1):\n",
    "            matrix[i,0] = i\n",
    "            operations[i][0] = insert\n",
    "            if i > 0 and j > 0:\n",
    "                subCost = matrix[i-1, j-1] if s1[i-1] == s2[j-1] else matrix[i-1, j-1] + subcost\n",
    "                insertCost = matrix[i, j-1] + inscost\n",
    "                deleteCost = matrix[i-1, j] + delcost\n",
    "                minCost = min(subCost, insertCost, deleteCost)\n",
    "                matrix[i,j] = minCost\n",
    "                if minCost == 0:\n",
    "                    operations[i][j] = noop\n",
    "                elif minCost == deleteCost:\n",
    "                    operations[i][j] = delete\n",
    "                elif minCost == insertCost:\n",
    "                    operations[i][j] = insert\n",
    "                elif minCost == subCost:\n",
    "                    operations[i][j] = sub\n",
    "    i = len(s1)\n",
    "    j = len(s2)\n",
    "    history = []\n",
    "    while j > 0 or i > 0:        \n",
    "        if delcost != np.inf:\n",
    "            if j == 0:\n",
    "                history.append(delete)\n",
    "                i -= 1\n",
    "                continue\n",
    "            if matrix[i-1][j-1] < matrix[i-1,j]:\n",
    "                history.append(noop)\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "            else:\n",
    "                history.append(delete)\n",
    "                i -= 1\n",
    "        elif inscost != np.inf:\n",
    "            if j == 0:\n",
    "                history.append(noop)\n",
    "                i -= 1\n",
    "                continue\n",
    "            if matrix[i-1][j-1] < matrix[i,j-1]:\n",
    "                history.append(noop)\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "            else:\n",
    "                history.append((insert,s2[j-1]))\n",
    "                #history.append(insert)\n",
    "                j -= 1\n",
    "    history.reverse()\n",
    "    return matrix, matrix[len(s1),len(s2)], history\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "30523\n"
     ]
    }
   ],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        with open(path, \"r\") as infile:\n",
    "            self.data = json.load(infile)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[\"Data\"])\n",
    "\n",
    "    def sample(self):\n",
    "        key = \"Question\" if random.random() > 0.5 else \"Answer\"\n",
    "        return self.data[\"Data\"][random.randint(0, len(self.data[\"Data\"]))][\"Question\"]\n",
    "    \n",
    "PLH = \"<PLH>\"\n",
    "TS = \"<s>\"\n",
    "TE = \"</s>\"\n",
    "\n",
    "class PlaceholderClassifier(nn.Module):\n",
    "    def __init__(self, hsz, max_placeholders=10):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(\n",
    "            hsz, max_placeholders,\n",
    "        )\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        return self.activation(self.dense(input))\n",
    "    \n",
    "class TokenClassifier(nn.Module):    \n",
    "    def __init__(self, hsz, vsz, max_seq_len):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dense = nn.Linear(\n",
    "            hsz, vsz\n",
    "        )\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, input, hidden=None):\n",
    "        return self.activation(self.dense(input))\n",
    "        \n",
    "class DeletionClassifier(nn.Module):    \n",
    "    def __init__(self, hsz):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(\n",
    "            hsz, 2,\n",
    "        )\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, input, hidden=None):\n",
    "        return self.activation(self.dense(input))\n",
    "dataset = QADataset(\"/virtualmachines/data/trivia_qa/qa/wikipedia-train.json\")\n",
    "dataset.sample()\n",
    "plh = tokenizer.encode(\"<PLH>\")[0]\n",
    "print(plh)\n",
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[7592, 1010, 2026, 2171, 2003], [2054, 2003, 2115]],\n",
       " [[7592, 1010, 2026, 2171, 2003], [2054, 2003, 2115, 30522, 30522]],\n",
       " [[0, 0, 0, 0, 0], [0, 0, 0, 1, 1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def deleted_indices_to_placeholders(deleted, max_placeholders):\n",
    "    i = 0\n",
    "    placeholders = []\n",
    "    num_deleted = 0\n",
    "    while True:\n",
    "        if i == len(deleted):\n",
    "            if num_deleted > 0:\n",
    "                placeholders.append(min(num_deleted, max_placeholders))\n",
    "            break\n",
    "        if deleted[i] == 1:\n",
    "            num_deleted += 1\n",
    "        else:\n",
    "            if num_deleted > 0:\n",
    "                placeholders.append(min(num_deleted, max_placeholders))\n",
    "            placeholders.append(0)\n",
    "            num_deleted = 0\n",
    "        i += 1\n",
    "    while len(placeholders) < len(deleted):\n",
    "        placeholders.append(0)\n",
    "    return torch.unsqueeze(torch.LongTensor(placeholders), 0)\n",
    "\n",
    "\n",
    "delete_random([tokenizer.encode(\"Hello, my name is \"), tokenizer.encode(\"What is your name?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[30522, 0, 1, 2], [5, 10, 12]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def delete_minimal(y, y_ground, pad=True, max_placeholders=0):\n",
    "    \"\"\"Apply the sequence of deletions from y that give the smallest possible Levenshtein distance from y_ground\n",
    "    Returns tensors of:\n",
    "    - the token sequence post-deletion\n",
    "    - the token sequence post-deletion, with PLH inserted at each deleted position\n",
    "    - the number of placeholders inserted at each post-deletion index\n",
    "    \"\"\"\n",
    "    batched = torch.zeros(len(y),)\n",
    "    \n",
    "    # if y_ground is longer than y, there is no sequence of deletes with a shorter distance\n",
    "    if len(y_ground) > len(y): \n",
    "        return y\n",
    "    \n",
    "    # calculate LD directly against tokens\n",
    "    deleted = []\n",
    "    matrix, dist, edits = ld(y.numpy(),y_ground.numpy(), subcost=np.inf, inscost=np.inf)\n",
    "    num_deleted = 0\n",
    "    for i in range(len(edits)):\n",
    "        if edits[i] == \"D\":\n",
    "            deleted.append(1)\n",
    "            num_deleted += 1\n",
    "        else:\n",
    "            deleted.append(0)\n",
    "    placeholders = deleted_indices_to_placeholders(deleted,max_placeholders=max_placeholders)\n",
    "    post_deletion, post_deletion_with_placeholders = deleted_boolean_to_tensors(y, deleted, num_deleted)    \n",
    "    return post_deletion, post_deletion_with_placeholders, torch.LongTensor([deleted])#,placeholders\n",
    "\n",
    "def pad(items, pad_len=0, pad_token=0):\n",
    "    if pad_len == 0:\n",
    "        pad_len = max([len(i) for i in items])\n",
    "    for i in items:\n",
    "        while len(i) < pad_len:\n",
    "            i.append(pad_token)\n",
    "    return items\n",
    "    \n",
    "def insert_minimal(y, y_ground,max_placeholders):\n",
    "    \"\"\"Apply the sequence of insertions to y resulting in the smallest possible Levenshtein distance from y_ground\n",
    "    Accepts tensor of:\n",
    "    - bsz * max_seq_len\n",
    "    Returns tensors of:\n",
    "    - size (n+1), where the value at position 0 <= i < n represents the number of PLH tags inserted at that position\n",
    "        - n is the number of tokens in y\n",
    "    - size(k) containing the indices of each inserted token, where k is the total number of tokens added\n",
    "    - size(n+k) containing the tokens of the entire post-insertion sequence\n",
    "    \"\"\"\n",
    "\n",
    "    batch_placeholders = []\n",
    "    batch_inserted = []\n",
    "    batch_new = []\n",
    "    for batch_idx in range(len(y)):\n",
    "        matrix, dist, edits = ld(y[batch_idx],y_ground[batch_idx], subcost=np.inf, delcost=np.inf)\n",
    "        inserted = 0\n",
    "        y_placeholders = []\n",
    "        y_inserted = []\n",
    "        y_new = []\n",
    "        y_idx = 0\n",
    "        i = 0\n",
    "        while i < len(edits):\n",
    "            if edits[i][0] == \"I\":\n",
    "                accum = 0\n",
    "                while i < len(edits) and edits[i][0] == \"I\":\n",
    "                    y_inserted.append(edits[i][1])\n",
    "                    y_new.append(edits[i][1])\n",
    "                    accum += 1\n",
    "                    i += 1\n",
    "                y_placeholders.append(min(accum,max_placeholders-1))\n",
    "            else:\n",
    "                y_placeholders.append(0)\n",
    "                y_new.append(y[batch_idx][y_idx])\n",
    "                i += 1\n",
    "                y_idx += 1\n",
    "        batch_placeholders.append(y_placeholders)\n",
    "        batch_inserted.append(y_inserted)\n",
    "        batch_new.append(y_new)\n",
    "    batch_placeholders = pad(batch_placeholders, pad_token=tokenizer.pad_token_id)\n",
    "\n",
    "    batch_new = pad(batch_new, pad_token=tokenizer.pad_token_id)\n",
    "    \n",
    "    return batch_placeholders, batch_inserted, torch.LongTensor(batch_new)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7592, 2026, 2171, 2003]]\n",
      "hello my <PLH>is\n",
      "hello my is\n",
      "[[0, 0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "foo = InsertionInput([tokenizer.encode(\"Hello my name is\")])\n",
    "print(foo.ground)\n",
    "print(tokenizer.decode(foo.post_deletion_with_placeholders[0]))\n",
    "print(tokenizer.decode(foo.post_deletion[0]))\n",
    "print(foo.num_placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [100 x 768], m2: [100 x 30523] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:752",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-231-59d8d37d44d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsertion_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsertion_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0minsertion_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInsertionInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hi my friend\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m \u001b[0mdeletion_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeletionInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minsertion_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTokenClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;31m#tokenizer.decode(deletion_input.post_insertion[0].tolist())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-231-59d8d37d44d5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, insertion_input, t_classifier, p_ins, max_inserts)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_insertion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m#print(encoded.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_insertion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_insertion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_insertion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-1385351f98ad>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDeletionClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [100 x 768], m2: [100 x 30523] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:752"
     ]
    }
   ],
   "source": [
    "class InsertionInput():\n",
    "    '''\n",
    "    Wraps tensors of;\n",
    "    - the original sequence\n",
    "    - the post-deletion sequence\n",
    "    - the post-deletion sequence (including placeholders)\n",
    "    - boolean indicating whether the tokens at index i was deleted\n",
    "    '''\n",
    "    def __init__(self, ground, p_del=0.25):\n",
    "        self.ground = ground\n",
    "        self.p_del = p_del\n",
    "        self.delete_random()\n",
    "    \n",
    "    def delete_random(self):\n",
    "        \"\"\"Deletes token(s) randomly from the passed (tokenized) string with probability p\n",
    "        Accepts:\n",
    "        - a list of token sequences bsz * pad_length\n",
    "        Returns tensors of:\n",
    "        - the token sequence post-deletion\n",
    "        - the token sequence post-deletion, with PLH inserted at each deleted position\n",
    "        - the number of placeholders inserted at each post-deletion index\n",
    "        \"\"\"\n",
    "        self.deleted_indices = []\n",
    "        self.post_deletion = []\n",
    "        self.post_deletion_with_placeholders = []\n",
    "        for entry in self.ground:\n",
    "            indices = []\n",
    "            post_deletion_with_placeholders = []\n",
    "            post_deletion = []\n",
    "            for token in entry:\n",
    "                if random.random() < self.p_del:\n",
    "                    indices.append(1)\n",
    "                    post_deletion_with_placeholders.append(plh)\n",
    "                else:\n",
    "                    indices.append(0)\n",
    "                    post_deletion.append(token)\n",
    "                    post_deletion_with_placeholders.append(token)\n",
    "            self.deleted_indices.append(indices)\n",
    "            self.post_deletion_with_placeholders.append(post_deletion_with_placeholders)\n",
    "            self.post_deletion.append(post_deletion)\n",
    "        self.num_placeholders = []\n",
    "        for entry in self.post_deletion_with_placeholders:\n",
    "            num_placeholders = []\n",
    "            accum = 0\n",
    "            for token in entry:\n",
    "                if token == plh:\n",
    "                    accum += 1\n",
    "                    continue\n",
    "                if accum > 0:\n",
    "                    num_placeholders.append(accum)\n",
    "                    accum = 0\n",
    "                num_placeholders.append(accum)\n",
    "            if accum > 0:\n",
    "                num_placeholders.append(accum)\n",
    "            self.num_placeholders.append(num_placeholders)\n",
    "        self.post_deletion = pad(self.post_deletion, pad_len=100)\n",
    "        self.num_placeholders = pad(self.num_placeholders, pad_len=100)\n",
    "        self.post_deletion_with_placeholders = pad(self.post_deletion_with_placeholders, pad_len=100)\n",
    "        self.num_placeholders = torch.LongTensor(self.num_placeholders)\n",
    "        self.post_deletion = torch.LongTensor(self.post_deletion)\n",
    "        self.post_deletion_with_placeholders = torch.LongTensor(self.post_deletion_with_placeholders)\n",
    "\n",
    "class DeletionInput():\n",
    "    \"\"\"Randomly inserts tokens into the sequence\n",
    "    Wraps tensors of:\n",
    "    - the post-insertion sequence\n",
    "    - the indices of the inserted tokens\n",
    "    \"\"\" \n",
    "    def __init__(self, insertion_input, t_classifier, p_ins=0.25, max_inserts=5):\n",
    "#        if v >= self.alpha and len(y0) != 0:\n",
    "#            y_del = y_ins\n",
    "#        else:\n",
    "        ground = insertion_input.ground.copy()\n",
    "        bsz = len(ground)\n",
    "        self.insertion_indices = [] \n",
    "        for batch_idx in range(bsz):\n",
    "            i = 0\n",
    "            insertion_indices = [0] * len(ground[batch_idx])\n",
    "            inserted = 0\n",
    "            while i < len(ground[batch_idx]):\n",
    "                #print(\"iter\")\n",
    "                if random.random() < p_ins and inserted < max_inserts:\n",
    "                    ground[batch_idx] = ground[batch_idx][:i] + [plh] + ground[batch_idx][i:]\n",
    "                    insertion_indices[i-inserted] = 1\n",
    "                    inserted += 1\n",
    "                i += 1\n",
    "            self.insertion_indices.append(insertion_indices)\n",
    "        self.post_insertion = torch.LongTensor(pad(ground, pad_len=100))\n",
    "        encoded = encoder(self.post_insertion)[0]\n",
    "        logits = t_classifier(encoded)\n",
    "        self.post_insertion = torch.argmax(logits,dim=2)\n",
    "        self.post_insertion = torch.LongTensor(self.post_insertion)\n",
    "        self.insertion_indices = pad(self.insertion_indices,pad_len=100)\n",
    "        self.insertion_indices = torch.LongTensor(self.insertion_indices)\n",
    "insertion_input = InsertionInput([tokenizer.encode(\"Hi my friend\")])\n",
    "deletion_input = DeletionInput(insertion_input, TokenClassifier(100, len(tokenizer), 20))\n",
    "#tokenizer.decode(deletion_input.post_insertion[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, vocab_size, hsz=768, lr=0.0001, max_placeholders=10, alpha=0, beta=0):\n",
    "        super().__init__()\n",
    "        self.p_classifier = PlaceholderClassifier(hsz, max_placeholders=max_placeholders)\n",
    "        self.t_classifier = TokenClassifier(hsz, vocab_size, 20)\n",
    "        self.d_classifier = DeletionClassifier(hsz)        \n",
    "        self.dataset = QADataset(\"/virtualmachines/data/trivia_qa/qa/wikipedia-train.json\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoder = encoder\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.max_placeholders = max_placeholders             \n",
    "        \n",
    "        self.alpha = 0.5\n",
    "        self.beta = 0.5\n",
    "        self.p_loss = nn.CrossEntropyLoss()\n",
    "        self.t_loss = nn.CrossEntropyLoss()\n",
    "        self.d_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.optims = {\n",
    "            'p_classifier': optim.SGD(self.p_classifier.parameters(), lr=lr),\n",
    "            't_classifier': optim.SGD(self.t_classifier.parameters(), lr=lr),\n",
    "            'd_classifier': optim.SGD(self.d_classifier.parameters(), lr=lr),\n",
    "        }\n",
    "        \n",
    "        self.step = 0\n",
    "        self.loss = 0\n",
    "                        \n",
    "    def sample(self, bs=10, pad_to=30):\n",
    "        '''Sample an observation and return tensor tuples:\n",
    "        \n",
    "        2) (a) 1(a), but with PLH replacing each deleted token. For input to the token insertion classifier \n",
    "           (b) the number of placeholders tokens to insert at each index in 1(a) (for the placeholder classifier loss)\n",
    "        3) (a) the observation perturbed with insertion (tokens), for input to the token deletion classifier \n",
    "           (b) 2(a) (for the token insertion classifier loss)\n",
    "        '''\n",
    "        \n",
    "        v = random.random()\n",
    "        \n",
    "        # sample an untokenized string\n",
    "        y_ground = [self.dataset.sample() for i in range(bs)]\n",
    "        # tokenize/encode\n",
    "        encoded = [self.tokenizer.encode(s) for s in y_ground]\n",
    "        \n",
    "        insertion_input = InsertionInput(encoded)\n",
    "        deletion_input = DeletionInput(insertion_input, self.t_classifier)\n",
    "    \n",
    "        return insertion_input, deletion_input\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for optimizer in self.optims.values():\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    def update_params(self):\n",
    "        for optimizer in self.optims.values():\n",
    "            optimizer.step()\n",
    "            \n",
    "    def encode_list(self, tokens):\n",
    "        if type(tokens) == list:\n",
    "            return encoder(torch.LongTensor([tokens]))[0]\n",
    "        return encoder(tokens)[0]\n",
    "            \n",
    "    def pretty_print(self, label, tokens):\n",
    "        if type(tokens) == torch.Tensor:\n",
    "            tokens = tokens.tolist()\n",
    "        pretty = \"%s \\n %s\" % (label, tokenizer.decode(tokens))\n",
    "        pretty = pretty.replace(\"[PAD]\",\"\")\n",
    "        print(pretty)\n",
    "        \n",
    "    def decode_step(self):\n",
    "        with torch.no_grad():\n",
    "            self.p_classifier.eval()\n",
    "            self.t_classifier.eval()\n",
    "            self.d_classifier.eval()\n",
    "\n",
    "            insertion_input, deletion_input = self.sample(bs=1)\n",
    "            self.pretty_print(\"Ground truth\", insertion_input.ground[0])\n",
    "            self.pretty_print(\"Ground truth post-deletion\", insertion_input.post_deletion[0].tolist())\n",
    "\n",
    "            step = 0\n",
    "            max_steps = 10\n",
    "            last = insertion_input.post_deletion\n",
    "            ground = last\n",
    "            \n",
    "            while True:\n",
    "                print(\"Decode step %d\" % step)\n",
    "            \n",
    "                if step > max_steps:\n",
    "                    ground = last\n",
    "                    break\n",
    "                \n",
    "                if step > 0 and (last.size() == ground.size() and torch.all(last == ground)):\n",
    "                    break\n",
    "                \n",
    "                step += 1    \n",
    "                \n",
    "                # run a deletion pass\n",
    "                preds_deletes = self.d_classifier(self.encode_list(ground))\n",
    "                if preds_deletes.size(1) > 0:\n",
    "                    deletions = torch.argmax(preds_deletes,2)\n",
    "                    assert ground.size(1) >= deletions.size(1)\n",
    "                    deleted = [ground[0,i].item() for i in range(deletions.size(1)) if deletions[0,i] == 0]\n",
    "                    ground = torch.LongTensor([deleted])\n",
    "                    self.pretty_print(\"Post-deletion\", deleted)\n",
    "                else:\n",
    "                    print(\"No deletions\")\n",
    "                    \n",
    "                preds_placeholders = self.p_classifier(self.encode_list(ground))\n",
    "                # then run a placeholder pass                    \n",
    "                if preds_placeholders.size(1) == 0:\n",
    "                    print(\"No placeholders\")\n",
    "                    continue\n",
    "                    \n",
    "                placeholders = torch.argmax(preds_placeholders,2)\n",
    "                \n",
    "                reconstructed = []\n",
    "                added_placeholders = 0\n",
    "                for i in range(ground.size(1)):\n",
    "                    for j in range(min(self.max_placeholders, placeholders[0,i])):\n",
    "                        reconstructed.append(plh)\n",
    "                        added_placeholders += 1\n",
    "                    reconstructed.append(ground[0,i].item())\n",
    "\n",
    "                self.pretty_print(\"Post-placeholders\", reconstructed)\n",
    "                      \n",
    "                # then an insertion pass\n",
    "                preds_inserts = self.t_classifier(self.encode_list(reconstructed))\n",
    "                inserts = torch.argmax(preds_inserts, 2)\n",
    "                \n",
    "                output = []\n",
    "                for i in range(len(reconstructed)):\n",
    "                    if reconstructed[i] == plh:\n",
    "                        output.append(inserts[0,i].item())\n",
    "                    else:\n",
    "                        output.append(reconstructed[i])\n",
    "                self.pretty_print(\"Post-insert\", output)\n",
    "                last = torch.LongTensor([output])\n",
    "            if last is not None:\n",
    "                print(last[0].tolist())\n",
    "                print(tokenizer.decode(last[0].tolist()))\n",
    "            else:\n",
    "                print(\"Couldn't decode\")\n",
    "            \n",
    "    def train_step(self):\n",
    "        loss = 0\n",
    "        self.zero_grad()\n",
    "        self.p_classifier.train()\n",
    "        self.t_classifier.train()\n",
    "        self.d_classifier.train()\n",
    "        \n",
    "        insertion_input, deletion_input = self.sample(bs=1)\n",
    "        \n",
    "        preds_deletes = self.d_classifier(encoder(deletion_input.post_insertion)[0])\n",
    "        preds_placeholders = self.p_classifier(encoder(insertion_input.post_deletion)[0])\n",
    "        preds_inserts = self.t_classifier(encoder(insertion_input.post_deletion_with_placeholders)[0])\n",
    "        \n",
    "        loss = self.d_loss(torch.transpose(preds_deletes, 1, 2), deletion_input.insertion_indices)\n",
    "        loss += self.p_loss(torch.transpose(preds_placeholders, 1, 2), insertion_input.num_placeholders)\n",
    "        loss += self.t_loss(torch.transpose(preds_inserts,1,2), insertion_input.post_deletion)\n",
    "        self.step += 1\n",
    "        self.loss += loss\n",
    "        if self.step % 10 == 0:\n",
    "            print(self.step)\n",
    "        if self.step % 50 == 0:\n",
    "            print(self.step)\n",
    "            print(self.loss / 50)\n",
    "            self.loss = 0\n",
    "            self.decode_step()\n",
    "        \n",
    "        loss.backward()\n",
    "        self.update_params()\n",
    "\n",
    "model = Model(len(tokenizer))\n",
    "for i in range(10000):\n",
    "    model.train_step()\n",
    "model.decode_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decode step 0\n",
      "tensor([[0, 0, 0, 0]], device='cuda:0')\n",
      "post deleted\n",
      "i are go to\n",
      "Decode step 1\n",
      "tensor([[0, 0, 0, 0]], device='cuda:0')\n",
      "post deleted\n",
      "##nxnxnxnx\n",
      "Decode step 2\n",
      "tensor([[0, 0, 0, 0]], device='cuda:0')\n",
      "post deleted\n",
      "##vancesnxvancesnx\n",
      "Decode step 3\n",
      "tensor([[0, 0, 0, 0]], device='cuda:0')\n",
      "post deleted\n",
      "##vancesvancesvancesnx\n",
      "Decode step 4\n",
      "tensor([[0, 0, 0, 0]], device='cuda:0')\n",
      "post deleted\n",
      "##vancesvancesvancesvances\n",
      "Decode step 5\n",
      "tensor([[0, 0, 0, 0]], device='cuda:0')\n",
      "post deleted\n",
      "midtownvancesvancesvances\n",
      "Decode step 6\n",
      "[27219, 26711, 26711, 26711]\n",
      "midtownvancesvancesvances\n"
     ]
    }
   ],
   "source": [
    "def decode(self, text):\n",
    "        \n",
    "    self.p_classifier.eval()\n",
    "    self.t_classifier.eval()\n",
    "    self.d_classifier.eval()\n",
    "\n",
    "    step = 0\n",
    "    max_steps = 10\n",
    "    \n",
    "    y_last = None\n",
    "    y_ground = torch.LongTensor([tokenizer.encode(text)])\n",
    "    y_ground_enc = encoder(y_ground)[0]\n",
    "    \n",
    "    while True:\n",
    "        print(\"Decode step %d\" % step)\n",
    "        step += 1\n",
    "        if step > max_steps:\n",
    "            y_ground = y_last\n",
    "            break\n",
    "        if y_last is not None:\n",
    "            if (y_last.size() == y_ground.size() and torch.all(y_last == y_ground)):\n",
    "                break         \n",
    "            y_ground = y_last\n",
    "                \n",
    "        # deletion first\n",
    "        preds_deletes = self.d_classifier(y_ground_enc).cuda()\n",
    "        if preds_deletes.size(1) == 0:\n",
    "            continue\n",
    "\n",
    "        deletions = torch.argmax(preds_deletes,2)\n",
    "        print(deletions)\n",
    "\n",
    "        deleted = [y_ground[0,i].item() for i in range(deletions.size(1)) if deletions[0,i] == 0]\n",
    "\n",
    "        y_ground = tokenizer.decode(deleted)\n",
    "        print(\"post deleted\")\n",
    "        print(y_ground)\n",
    "\n",
    "        # then placeholder\n",
    "        y_ground = encoder(torch.LongTensor([deleted]))[0]\n",
    "\n",
    "        preds_placeholders = self.p_classifier(y_ground).cuda()\n",
    "        if preds_placeholders.size(1) == 0:\n",
    "            y_ground = torch.LongTensor([deleted])\n",
    "            continue\n",
    "        placeholders = torch.argmax(preds_placeholders,2)\n",
    "\n",
    "        reconstructed = []\n",
    "        for i in range(placeholders.size(1)):\n",
    "            for j in range(placeholders[0,i].item()):\n",
    "                reconstructed.append(plh)\n",
    "            reconstructed.append(deleted[i])\n",
    "\n",
    "        y_ground = torch.LongTensor([reconstructed])\n",
    "\n",
    "        # then inserts\n",
    "        inserts = self.t_classifier(encoder(torch.LongTensor([reconstructed]))[0]).cuda()\n",
    "        inserts = torch.argmax(inserts, 2)\n",
    "        j = 0\n",
    "\n",
    "        for i in range(len(reconstructed)):\n",
    "            if reconstructed[i] == plh:\n",
    "                if i < len(reconstructed):\n",
    "                    reconstructed[i] = inserts[0,j].item()\n",
    "                else:\n",
    "                    reconstructed.append(inserts[0,j].item())\n",
    "                j += 1\n",
    "        y_last = torch.LongTensor([reconstructed])\n",
    "    if y_last is not None:\n",
    "        print(y_last[0].tolist())\n",
    "        print(tokenizer.decode(y_last[0].tolist()))\n",
    "    else:\n",
    "        print(\"y_last none\")\n",
    "decode(model, \"I are go to\")\n",
    "#decode(model, \"She am my mother\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
